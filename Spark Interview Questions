1. Why Spark is Faster Than Hadoop? Hadoop Vs spark

HADOOP:
Hadoop is a distributed file system (HDFS) while Spark is a compute engine running on top of Hadoop or your local file system. 
Spark however is faster than MapReduce which was the first compute engine created when HDFS was created.
So, when Hadoop was created, there were only two things. HDFS where data is stored and MapReduce which was the only compute engine on HDFS. To understand how Spark is faster than MapReduce, you need to understand how both MapReduce and Spark works.

When a MR job starts, the first step is to read data from disk and run mappers. The output of mappers is stored back on disk. 
Then Shuffle and sort step starts and reads the mapper output from disk and after shuffle and sort completes, it stores
the result back on disk (there is actually some network traffic also when keys for Reduce step are gathered on 
same node but that's true for Spark also, so let's focus on the disk step only). Then finally the reduce step starts, 
reads the output from shuffle and sort step and finally stores the result back in HDFS.
That's six disk accesses to complete the job. Most Hadoop clusters have 7200 RPM disks which are ridiculously slow.


SPARK :
Now, here is how Spark works. Like MapReduce job needs mappers and reducers, Spark has two types of processes. 
One is transformation and other is action. When you write a Spark job, it consists of a number of transformations and a 
few actions. When Spark job starts, it creates a DAG (Directed acyclic graph) of the job (steps it is supposed to run as 
part of the job). Then when a job starts, it looks at the DAG and assume the first 5 steps are transformations. It 
remembers the steps (the DAG) but doesn't really go to disk to perform the transformations. Then it encounters action. 
At that point a Spark job goes to disk, performs the first transformation, keeps the result of transformation in memory, 
performs the second transformation, keeps the result in memory and so on until all the steps complete. The only time it 
goes back to disk is to write the output of the job. So, two accesses to disk. This makes Spark faster. There are other 
things in Spark which makes it faster than MapReduce. For example, a rich set of API which enables to accomplish in 
one Spark job what might require two or more MapReduce jobs running one after the other. Imagine, how slow that would be. 

2. Which language to choose and Why? Scala vs Python

Performance: Scala wins. Python is 10X slower than JVM languages. If your Python code just calls Spark libraries, 
you'll be OK. But if your Python code makes a lot of processing, it will run slower than the Scala equivalent.

Ease of use: Scala wins. Spark itself is built on Scala. Things are "more natural" using Scala. 

Scala has strong static types. Errors are raised at the compilation stage. It makes your development process 
easier especially in big projects.

Scala is based on JVM so it's native for Hadoop. Hadoop is important because Spark was made on the top of the Hadoop's 
filesystem HDFS. Python interacts with Hadoop services very badly, so developers have to use 3rd party libraries (like hadoopy).
Scala interacts with Hadoop via native Hadoop's API in Java. 
That's why it's very easy to write native Hadoop applications in Scala.


3. Explain about the Apache Spark Architecture

Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).

Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark’s
own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, S
park acquires executors on nodes in the cluster, which are processes that run computations and store data for your 
application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors.
Finally, SparkContext sends tasks to the executors to run.


4. What do you understand by Spark Execution Model
5. Brief about spark internals, Spark Session vs Spark Context
SPARKCONTEXT:
Spark Context is a conduit( a channel) to access all spark functionality: ONly single SparkCOntext exists per JVM.
The spark driver Program uses it to connect to the cluster Manager to communicate and submit spark job.
It allows you to programmatically adjust Spark configuration parameers and through SparkCOntext
the driver can Instantaite Other context such as SQLCOntext, HiveCOntext,SparkStreamingCOntext to progrom spark.

SPARKSESSION:
SparkSession can access all of SPark fucntiona;ity through a singleunified point of entry. as well as making it simplere to access
spark functiona;ity such as DATAFRAMES and Data Sets ,catalogues and spark COnfiguration, it also subsumes the underlying contexts
to manipulate data.

https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html

6. Spark Driver vs Spark Executor
Spark Executor:
 A Spark Executor is JVM container with an allocated amount of cores and memeory on which spark runs its task.
 each worked node launces its own spark executor, with a configurable number of cores (or threads).
 Besides executing Spark task, an Executor also stores and caches all data partitions in its memory.
 
 Spark Driver:
 Once it ges INformation from the spark Master of all the workers in the cluster and where they are, the driver progrom
 distributes spark tasks to each worker's executors. the driver also receives computes results from each executor's tasks.
 
 
7. Executor vs Executor core
8. Yarn client mode vs cluster mode

 both strategies use the cluster to distribute tasks; the difference is where the "driver program" runs: 
 
 CLUSTER MODE:
 
 In yarn-cluster mode, the driver runs in the Application Master.
 This means that the same process is responsible for both driving the application and requesting resources from YARN, and this process runs inside a YARN container. 
 The client that starts the app doesn’t need to stick around for its entire lifetime.
 The yarn-cluster mode is not well suited to using Spark interactively, 
 
 CLIENT MODE:
 
 but the yarn-client mode is. Spark applications that require user input, 
 like spark-shell and PySpark, need the Spark driver to run inside the client process that initiates the Spark application. 
 In yarn-client mode, the Application Master is merely present to request executor containers from YARN. 
 The client communicates with those containers to schedule work after they start:


9. What is RDD and what do you understand by partitions?
10. What do you understand by Fault tolerance in Spark?
11. Spark vs Yarn Fault tolerance
12. Why Lazy evaluation is important in Spark?
real computations happens when we call the action. 
13. Transformations vs actions

14. Map vs FlatMap

map and flatmap both are transformations . map  gets row as input and generate a tow as output 
15. Spark Map vs Map Partition
map converts each element of the source RDD into a single element of the result  RDD by applying function.
Map partition convert convert each partition of the source RDD into multiple elements of the result.

16. Wide vs Narrow transformations

narrow transformations are esult of map(), filter. the elements that need to be compute records in
single paritions will be presnt in single parent rdd

wide transormations are results of reducebykey , groupbykey
17. Reduce by key vs Group by key
REDUCE BYKEY:

Data is combined so that at each partition there should be at least one value for each key. 
And then shuffle happens and it is sent over the network to some particular executor for some action such as reduce.
GroupBYKEY:
GroupByKey - groupByKey([numTasks])

It doesn't merge the values for the key but directly the shuffle process happens and here lot of data gets sent 
to each partition, almost same as the initial data.

And the merging of values for each key is done after the shuffle. Here lot of data stored on final worker node so 
resulting in out of memory issue.

18. What do you understand by Spark Lineage
set of transformation operations that are used to create a final transformation that we are intesrested.
19. Spark Lineage vs Spark DAG

spark lineage is logical plan

spark DAG is a physical plan
20. Spark cache vs Spark persist
Spark Cache and persist are optimization techniques for iterative and interactive 
Spark applications to improve the performance of the jobs or applications.

Both caching and persisting are used to save the Spark RDD, Dataframe and Dataset’s. But, the difference is, RDD cache() 
method default saves it to memory (MEMORY_ONLY) whereas persist() method is used to store it to user-defined storage level
cost effiecient , time efficeint , execution time

21. What do you understand by AggregateByKey and CombineByKey?

ReduceByKey and GroupByKey:

While both reducebykey and groupbykey will produce the same answer, the reduceByKey example works much better on a large dataset.
That's because Spark knows it can combine output with a common key on each partition before shuffling the data.

CombineByKey:

combineByKey can be used when you are combining elements but your return type differs from your input value type. 
Spark combineByKey is a transformation operation on PairRDD (i.e. RDD with key/value pair). 
It is a wider operation as it requires shuffle in the last stage.


aggregateByKey:
AggregateByKey is same like combineByKey and there is slight difference in functioning and arguments. The aggregateByKey function is used to aggregate the values for each key and adds the potential to return a different value type.

The three parameters of aggregateByKey function,

zeroValue: As we are finding maximum marks out of all subjects we should use Double.MinValue (which is also known as an accumulator)

seqOp: Sequential operation is an operation of finding maximum marks (operation at each partition level data)

combOp: Combiner operation is an operation of finding maximum marks from two values (operation on aggregated data of all partitions)


22. Briefly explain about Spark Accumulator?

Spark Accumulators are shared variables which are only “added” through an associative and commutative operation and are 
used to perform counters (Similar to Map-reduce counters) or sum operations

Spark by default provides accumulator methods for long, double and collection types. All these methods are present in SparkContext 
class and return LongAccumulator, DoubleAccumulator, and CollectionAccumulator respectively.

https://sparkbyexamples.com/spark/spark-accumulators/

23. What do you mean by Broadcast variables?

In Spark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, spark distributes broadcast
variables to the machine using efficient broadcast algorithms to reduce communication costs.

https://sparkbyexamples.com/spark/spark-broadcast-variables/

24. Spark UDF functions, Why one should avoid UDF?
25. Why one should avoid RDDs, what is the alternative?
26. What are the benefits of a data frame?
27. What do you understand by Vectorized UDF?
28. Which one is better and when you should use, RDDs, Dataframe and Datasets?
29. Why Spark Dataset is typesafe?
30. Explain about Repartition and Coalesce.
31. How to read JSON from Spark?
32. Explain about Spark WIndow functions and it’s usage.
33. Spark Rank vs Dense Rank
34. Partitions vs Bucketing
35. Explain about catalyst optimizer
36. Stateless vs Stateful transformations
37. StructType and StructField
38. Explain about Apache parquet
39. What do you understand by CBO, Spark Cost Based Optimizer?
40. Explain Broadcast variable and shared variable with examples
41. Have you ever worked on Spark performance tuning and executor tuning
42. Explain Spark Join without shuffle
43. Explain about Paired RDD
44. Cache vs Persist in Spark UI
45. Why one should avoid groupBy?
46. How to decide the number of partitions in a data frame?
47. What is DAG? Explain in details.
48. Persistence vs Broadcast in Spark
49. Partition pruning and predicate pushdown
50. Fold vs reduce in Spark
51. Explain the interlinking of Pyspark and Apache Arrow
52. Explain about bucketing in Spark SQL
53. Explain dynamic resource allocation in Spark
54. Why fold-left and fold-right are not supported in Spark?
55. How to decide the number of executors and memory for any spark job?
56. Different types of cluster managers in spark
57. Can you explain how to minimize data transfers while working with Spark?
58. What are the different levels of persistence in Spark?
59. What is the function of filer()?
60. Define Partitions in Apache Spark?
61. What is the difference between reducing () and take() function?
62. Define YARN in Spark?
63. Can we trigger automated clean-ups in Spark?
64. What is another method than “Spark.cleaner.ttl” to trigger automated clean-ups in Spark?
65. What is the role of Akka in Spark?
66. Define SchemaRDD in Apache Spark RDD
67. What is a Spark Driver?



